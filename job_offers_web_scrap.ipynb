{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Job Offers web-scrapping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automation 1: Log-in Action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.79M/6.79M [00:01<00:00, 5.60MB/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    > This section automates the process of Logging-in to Linkedin.\n",
    "    > Make sure to use a valid Email Address and password.\n",
    "    > Selenium will open a chrome window which you can interact with.\n",
    "    > Once logged in, the site may ask to verify if you are human, you can use the pop-up window to solve the captcha or insert the access code.\n",
    "    > If any error arises, make sure to check the relevant driver.find_elements, as with time these are likely to be updated by Linkedin.\n",
    "'''\n",
    "\n",
    "# Creating a webdriver instance to log into LinkedIn using selenium required Chrome version\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Opening linkedIn's login page\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "\n",
    "# waiting for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Accessing username field\n",
    "username = driver.find_element_by_id(\"username\")\n",
    "\n",
    "# Enter Your Email Address\n",
    "username.send_keys(\"email@emailprovider.com\")\n",
    "\n",
    "# Accessing password fiel\n",
    "pword = driver.find_element_by_id(\"password\")\n",
    "\n",
    "# Enter Your Password\n",
    "pword.send_keys(\"PASSWORD\")\n",
    "\n",
    "# Clicking on the log in button\n",
    "driver.find_element_by_xpath(\"//button[@type='submit']\").click()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automation 2: Navigate to job section and insert job queries (Job Title and location)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "'''\n",
    "    > This section automates the process to access the required job listings.\n",
    "    > If any error arises, make sure to check the relevant driver.find_elements as with time these are likely to be updated by Linkedin\n",
    "'''\n",
    "\n",
    "jobTitle = 'computer science'                   # Insert job title to query\n",
    "jobLocation = 'United Arab Emirates'            # Insert job location to query\n",
    "\n",
    "# Switching to Jobs page to be able to query location\n",
    "jobs = driver.find_element_by_xpath('//*[@id=\"global-nav\"]/div/nav/ul/li[3]')\n",
    "jobs.click()\n",
    "\n",
    "# Clicking on search bar\n",
    "searchBar = driver.find_element_by_class_name('jobs-search-box__keywords-label')\n",
    "searchBar.click()\n",
    "search_src = driver.page_source\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#Adds Job Title\n",
    "\n",
    "patternCriteria = re.compile(r'jobs-search-box-keyword-id-ember[0-9]*')\n",
    "matchCriteria = patternCriteria.search(search_src).group(0)\n",
    "searchKeyWords = driver.find_element_by_id(matchCriteria).send_keys(jobTitle)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#Adds Location and hits enter\n",
    "patternLocation = re.compile(r'jobs-search-box-location-id-ember[0-9]*')\n",
    "matchLocation = patternLocation.search(search_src).group(0)\n",
    "CountryKeyWords = driver.find_element_by_id(matchLocation).send_keys(jobLocation)\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_id(matchLocation).send_keys(Keys.ENTER)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automation 3: extract URLs from job offers list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    > This process will automatically scroll through the different pages available and extract all the URLs from the different job posts.\n",
    "    > Upon completion and as a checkpoint of the process, all links will be saved to disk in a CSV format in the project directory.\n",
    "    > Different validations are in place to have an overview of the extraction process and supervise the workflow.\n",
    "    > If any error arises, make sure to check the relevant driver.find_elements as with time these are likely to be updated by Linkedin.\n",
    "'''\n",
    "# Settings: Scroll through Pagination\n",
    "findPages = driver.find_elements_by_class_name(\"artdeco-pagination__indicator.artdeco-pagination__indicator--number\")\n",
    "totalPages = findPages[len(findPages)-1].text\n",
    "totalPagesInt = int(re.sub(r\"[^\\d.]\", \"\", totalPages))\n",
    "\n",
    "# Variable Setup for URL Extraction Loop\n",
    "indexPage = 1\n",
    "jobLoopTTime = 0\n",
    "job_links = []\n",
    "randTime = random.randint(1,6)\n",
    "extractedURL = []\n",
    "final_extractedURL = []\n",
    "\n",
    "# URL Extraction Loop\n",
    "for page in  range(0,totalPagesInt):\n",
    "    jobLoop_start_time = time.time()\n",
    "    time.sleep(randTime)\n",
    "\n",
    "    # Scroll down to load all available jobs to reach job list bottom line\n",
    "    jobsBlock = driver.find_element_by_class_name('scaffold-layout__list-container')  #Indicates container of job list\n",
    "    jobsScaffold = jobsBlock.find_elements_by_css_selector(\".jobs-search-results__list-item\")  #Indicates css selector for the job entries (can be pulled from first *li* element)\n",
    "    jobEntry = 0\n",
    "    for i in jobsScaffold:\n",
    "        jobEntry+= 1\n",
    "        print(f\"-- scrolling to job: {str(jobEntry)} --\")\n",
    "        driver.execute_script('arguments[0].scrollIntoView();', jobsScaffold[jobEntry-1])\n",
    "\n",
    "    #Scraps all links in job list\n",
    "    time.sleep(randTime)\n",
    "    job_src = driver.page_source\n",
    "    soup = BeautifulSoup(job_src, 'lxml')\n",
    "    for link in soup.find_all('a', {'class': 'job-card-list__title'}):\n",
    "        job_links.append(link.get('href'))\n",
    "        URL = link.get('href')\n",
    "        pageNumb = indexPage\n",
    "        extractedURL = dict(((i, eval(i)) for i in ('pageNumb', 'URL')))\n",
    "        final_extractedURL.append(extractedURL)\n",
    "    print(job_links)\n",
    "    print(extractedURL)\n",
    "    print(f\"links extracted: {str(len(job_links))}\")\n",
    "    print(f\"pages scrapped: {str(round(len(job_links) / 25))}\")\n",
    "    print(f\"Duplicate Links Status: {str('NO Duplicated links' if (len(job_links) == len(set(job_links))) else 'There ARE Duplicates')}\")\n",
    "\n",
    "    #Scrolls to next page\n",
    "    indexPage+= 1\n",
    "    getNextPage = driver.find_element_by_xpath(\"//button[@aria-label='Page \"+str(indexPage)+\"']\")\n",
    "    getNextPage.send_keys(Keys.RETURN)\n",
    "    time.sleep(randTime)\n",
    "    print(f\"Next page is: {str(indexPage)}\")\n",
    "    jobLoop_end_time = time.time()\n",
    "    jobLoopTTime+= jobLoop_end_time - jobLoop_start_time\n",
    "    print(f\"elapsed time: {str(round(jobLoopTTime,3))} seconds\")\n",
    "    print(\"________________\")\n",
    "\n",
    "print(f'Total Elapsed Time: {str(round(jobLoopTTime/60,3))} minutes')\n",
    "\n",
    "# Adding main URL to extracted links\n",
    "full_url = [('https://linkedin.com' + URL) for URL in job_links]\n",
    "\n",
    "#Saving to disk the extracted links as backup\n",
    "URLsExport = pd.DataFrame(set(full_url))\n",
    "URLsExport.to_csv(\"URLscrapped.csv\", index=False, header=False)\n",
    "\n",
    "'''\n",
    "    > Uncomment below lines for debugging duplicates, URLsExport_2 stores both the URL and the page where the link was extracted from.\n",
    "'''\n",
    "# URLsExport_2 = pd.DataFrame(final_extractedURL)\n",
    "#URLsExport_2.to_csv(\"URLscrapped_bypage.csv\", index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automation 4: Extract job offer details"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs loaded from file: 75 links\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    > This process will use the links saved in the CSV and not the ones stored in memory.\n",
    "'''\n",
    "#extract links from CSV\n",
    "full_url_csv = pd.read_csv('URLscrapped.csv', names=['link'])\n",
    "full_url_csv = full_url_csv['link'].tolist()\n",
    "print(f'Total URLs loaded from file: {len(full_url_csv)} links')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "'''\n",
    "    > Uncomment below section if you want to scrap the job details in batches.\n",
    "    > Recommended if you are modifying the code and want to test the updates.\n",
    "'''\n",
    "# Spliting links into different groups to run the scrap in batches\n",
    "\n",
    "#test_urls_1 = full_url_csv[0:100]\n",
    "#test_urls_2 = full_url_csv[101:200]\n",
    "#test_urls_3 = full_url_csv[201:300]\n",
    "#test_urls_4 = full_url_csv[301:400]\n",
    "#test_urls_5 = full_url_csv[401:500]\n",
    "#test_urls_6 = full_url_csv[501:600]\n",
    "#test_urls_7 = full_url_csv[601:700]\n",
    "#test_urls_8 = full_url_csv[701:800]\n",
    "#test_urls_9 = full_url_csv[801:900]\n",
    "#test_urls_10 = full_url_csv[901:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#variable to insert dictionary of elements extracted\n",
    "final_dict = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links scrapped: 1\n",
      "overall completion: %1.33\n",
      "elapsed time: 9.47 seconds\n",
      "----------------------------\n",
      "links scrapped: 2\n",
      "overall completion: %2.67\n",
      "elapsed time: 19.03 seconds\n",
      "----------------------------\n",
      "links scrapped: 3\n",
      "overall completion: %4.0\n",
      "elapsed time: 28.27 seconds\n",
      "----------------------------\n",
      "links scrapped: 4\n",
      "overall completion: %5.33\n",
      "elapsed time: 37.31 seconds\n",
      "----------------------------\n",
      "links scrapped: 5\n",
      "overall completion: %6.67\n",
      "elapsed time: 46.43 seconds\n",
      "----------------------------\n",
      "links scrapped: 6\n",
      "overall completion: %8.0\n",
      "elapsed time: 55.51 seconds\n",
      "----------------------------\n",
      "links scrapped: 7\n",
      "overall completion: %9.33\n",
      "elapsed time: 64.59 seconds\n",
      "----------------------------\n",
      "links scrapped: 8\n",
      "overall completion: %10.67\n",
      "elapsed time: 73.81 seconds\n",
      "----------------------------\n",
      "links scrapped: 9\n",
      "overall completion: %12.0\n",
      "elapsed time: 83.12 seconds\n",
      "----------------------------\n",
      "links scrapped: 10\n",
      "overall completion: %13.33\n",
      "elapsed time: 92.32 seconds\n",
      "----------------------------\n",
      "links scrapped: 11\n",
      "overall completion: %14.67\n",
      "elapsed time: 101.4 seconds\n",
      "----------------------------\n",
      "links scrapped: 12\n",
      "overall completion: %16.0\n",
      "elapsed time: 110.47 seconds\n",
      "----------------------------\n",
      "links scrapped: 13\n",
      "overall completion: %17.33\n",
      "elapsed time: 119.62 seconds\n",
      "----------------------------\n",
      "links scrapped: 14\n",
      "overall completion: %18.67\n",
      "elapsed time: 128.84 seconds\n",
      "----------------------------\n",
      "links scrapped: 15\n",
      "overall completion: %20.0\n",
      "elapsed time: 138.1 seconds\n",
      "----------------------------\n",
      "links scrapped: 16\n",
      "overall completion: %21.33\n",
      "elapsed time: 147.22 seconds\n",
      "----------------------------\n",
      "links scrapped: 17\n",
      "overall completion: %22.67\n",
      "elapsed time: 156.42 seconds\n",
      "----------------------------\n",
      "links scrapped: 18\n",
      "overall completion: %24.0\n",
      "elapsed time: 165.51 seconds\n",
      "----------------------------\n",
      "links scrapped: 19\n",
      "overall completion: %25.33\n",
      "elapsed time: 174.59 seconds\n",
      "----------------------------\n",
      "links scrapped: 20\n",
      "overall completion: %26.67\n",
      "elapsed time: 183.73 seconds\n",
      "----------------------------\n",
      "links scrapped: 21\n",
      "overall completion: %28.0\n",
      "elapsed time: 192.92 seconds\n",
      "----------------------------\n",
      "links scrapped: 22\n",
      "overall completion: %29.33\n",
      "elapsed time: 202.04 seconds\n",
      "----------------------------\n",
      "links scrapped: 23\n",
      "overall completion: %30.67\n",
      "elapsed time: 211.19 seconds\n",
      "----------------------------\n",
      "links scrapped: 24\n",
      "overall completion: %32.0\n",
      "elapsed time: 220.3 seconds\n",
      "----------------------------\n",
      "links scrapped: 25\n",
      "overall completion: %33.33\n",
      "elapsed time: 229.38 seconds\n",
      "----------------------------\n",
      "links scrapped: 26\n",
      "overall completion: %34.67\n",
      "elapsed time: 238.45 seconds\n",
      "----------------------------\n",
      "links scrapped: 27\n",
      "overall completion: %36.0\n",
      "elapsed time: 247.66 seconds\n",
      "----------------------------\n",
      "links scrapped: 28\n",
      "overall completion: %37.33\n",
      "elapsed time: 256.78 seconds\n",
      "----------------------------\n",
      "links scrapped: 29\n",
      "overall completion: %38.67\n",
      "elapsed time: 266.18 seconds\n",
      "----------------------------\n",
      "links scrapped: 30\n",
      "overall completion: %40.0\n",
      "elapsed time: 275.3 seconds\n",
      "----------------------------\n",
      "links scrapped: 31\n",
      "overall completion: %41.33\n",
      "elapsed time: 284.39 seconds\n",
      "----------------------------\n",
      "links scrapped: 32\n",
      "overall completion: %42.67\n",
      "elapsed time: 293.48 seconds\n",
      "----------------------------\n",
      "links scrapped: 33\n",
      "overall completion: %44.0\n",
      "elapsed time: 302.66 seconds\n",
      "----------------------------\n",
      "links scrapped: 34\n",
      "overall completion: %45.33\n",
      "elapsed time: 311.72 seconds\n",
      "----------------------------\n",
      "links scrapped: 35\n",
      "overall completion: %46.67\n",
      "elapsed time: 320.81 seconds\n",
      "----------------------------\n",
      "links scrapped: 36\n",
      "overall completion: %48.0\n",
      "elapsed time: 329.87 seconds\n",
      "----------------------------\n",
      "links scrapped: 37\n",
      "overall completion: %49.33\n",
      "elapsed time: 338.9 seconds\n",
      "----------------------------\n",
      "links scrapped: 38\n",
      "overall completion: %50.67\n",
      "elapsed time: 348.03 seconds\n",
      "----------------------------\n",
      "links scrapped: 39\n",
      "overall completion: %52.0\n",
      "elapsed time: 357.21 seconds\n",
      "----------------------------\n",
      "links scrapped: 40\n",
      "overall completion: %53.33\n",
      "elapsed time: 366.27 seconds\n",
      "----------------------------\n",
      "links scrapped: 41\n",
      "overall completion: %54.67\n",
      "elapsed time: 375.46 seconds\n",
      "----------------------------\n",
      "links scrapped: 42\n",
      "overall completion: %56.0\n",
      "elapsed time: 384.53 seconds\n",
      "----------------------------\n",
      "links scrapped: 43\n",
      "overall completion: %57.33\n",
      "elapsed time: 393.62 seconds\n",
      "----------------------------\n",
      "URL no longer available\n",
      "links scrapped: 44\n",
      "overall completion: %58.67\n",
      "elapsed time: 403.69 seconds\n",
      "----------------------------\n",
      "links scrapped: 45\n",
      "overall completion: %60.0\n",
      "elapsed time: 412.82 seconds\n",
      "----------------------------\n",
      "links scrapped: 46\n",
      "overall completion: %61.33\n",
      "elapsed time: 422.03 seconds\n",
      "----------------------------\n",
      "links scrapped: 47\n",
      "overall completion: %62.67\n",
      "elapsed time: 431.45 seconds\n",
      "----------------------------\n",
      "links scrapped: 48\n",
      "overall completion: %64.0\n",
      "elapsed time: 440.56 seconds\n",
      "----------------------------\n",
      "links scrapped: 49\n",
      "overall completion: %65.33\n",
      "elapsed time: 449.67 seconds\n",
      "----------------------------\n",
      "links scrapped: 50\n",
      "overall completion: %66.67\n",
      "elapsed time: 458.84 seconds\n",
      "----------------------------\n",
      "links scrapped: 51\n",
      "overall completion: %68.0\n",
      "elapsed time: 468.05 seconds\n",
      "----------------------------\n",
      "links scrapped: 52\n",
      "overall completion: %69.33\n",
      "elapsed time: 477.17 seconds\n",
      "----------------------------\n",
      "links scrapped: 53\n",
      "overall completion: %70.67\n",
      "elapsed time: 486.52 seconds\n",
      "----------------------------\n",
      "links scrapped: 54\n",
      "overall completion: %72.0\n",
      "elapsed time: 495.91 seconds\n",
      "----------------------------\n",
      "links scrapped: 55\n",
      "overall completion: %73.33\n",
      "elapsed time: 505.21 seconds\n",
      "----------------------------\n",
      "links scrapped: 56\n",
      "overall completion: %74.67\n",
      "elapsed time: 514.42 seconds\n",
      "----------------------------\n",
      "links scrapped: 57\n",
      "overall completion: %76.0\n",
      "elapsed time: 523.73 seconds\n",
      "----------------------------\n",
      "links scrapped: 58\n",
      "overall completion: %77.33\n",
      "elapsed time: 533.1 seconds\n",
      "----------------------------\n",
      "links scrapped: 59\n",
      "overall completion: %78.67\n",
      "elapsed time: 542.76 seconds\n",
      "----------------------------\n",
      "links scrapped: 60\n",
      "overall completion: %80.0\n",
      "elapsed time: 552.14 seconds\n",
      "----------------------------\n",
      "links scrapped: 61\n",
      "overall completion: %81.33\n",
      "elapsed time: 561.38 seconds\n",
      "----------------------------\n",
      "links scrapped: 62\n",
      "overall completion: %82.67\n",
      "elapsed time: 570.76 seconds\n",
      "----------------------------\n",
      "links scrapped: 63\n",
      "overall completion: %84.0\n",
      "elapsed time: 579.98 seconds\n",
      "----------------------------\n",
      "links scrapped: 64\n",
      "overall completion: %85.33\n",
      "elapsed time: 589.63 seconds\n",
      "----------------------------\n",
      "links scrapped: 65\n",
      "overall completion: %86.67\n",
      "elapsed time: 599.32 seconds\n",
      "----------------------------\n",
      "links scrapped: 66\n",
      "overall completion: %88.0\n",
      "elapsed time: 608.8 seconds\n",
      "----------------------------\n",
      "links scrapped: 67\n",
      "overall completion: %89.33\n",
      "elapsed time: 618.26 seconds\n",
      "----------------------------\n",
      "links scrapped: 68\n",
      "overall completion: %90.67\n",
      "elapsed time: 627.68 seconds\n",
      "----------------------------\n",
      "links scrapped: 69\n",
      "overall completion: %92.0\n",
      "elapsed time: 637.09 seconds\n",
      "----------------------------\n",
      "links scrapped: 70\n",
      "overall completion: %93.33\n",
      "elapsed time: 646.56 seconds\n",
      "----------------------------\n",
      "links scrapped: 71\n",
      "overall completion: %94.67\n",
      "elapsed time: 656.0 seconds\n",
      "----------------------------\n",
      "links scrapped: 72\n",
      "overall completion: %96.0\n",
      "elapsed time: 665.49 seconds\n",
      "----------------------------\n",
      "links scrapped: 73\n",
      "overall completion: %97.33\n",
      "elapsed time: 674.86 seconds\n",
      "----------------------------\n",
      "links scrapped: 74\n",
      "overall completion: %98.67\n",
      "elapsed time: 684.27 seconds\n",
      "----------------------------\n",
      "links scrapped: 75\n",
      "overall completion: %100.0\n",
      "elapsed time: 693.76 seconds\n",
      "----------------------------\n",
      "Total elapsed time: 11.56 minutes\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    > The following loop will automate the extraction process from the loaded URLs.\n",
    "    > Extracted details will be saved to lnkd_scrapping_jobs.csv\n",
    "    > Different validations are in place to have an overview of the extraction process and supervise the workflow.\n",
    "    > If any error arises, make sure to check the relevant driver.find_elements as with time these are likely to be updated by Linkedin.\n",
    "    > If a link is no longer available, the fields for that given URL will be saved as empty fields and status will be set to: no longer active.\n",
    "'''\n",
    "# Loop to extract details from URL page\n",
    "start_time_cell = time.time()\n",
    "indexPage = 0\n",
    "jobLoopTTime = 0\n",
    "job_links = []\n",
    "randTime = random.randint(1,6)\n",
    "jobPageTime = 0\n",
    "\n",
    "for links in full_url_csv:\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        driver.get(links)\n",
    "        time.sleep(randTime)\n",
    "        jobDetails_src = driver.page_source\n",
    "        driver.find_element_by_class_name(\"artdeco-card__action\").click()\n",
    "        soup = BeautifulSoup(jobDetails_src, 'lxml')\n",
    "\n",
    "        jobURL = links\n",
    "        jobTitle = soup.find('h1', {'class': 'jobs-unified-top-card__job-title'}).text.strip()\n",
    "        jobDescription = soup.find('div', {'class': 'jobs-description-content__text--stretch'}).text.strip()\n",
    "        URLStatus = 'Active'\n",
    "        try:\n",
    "            jobCompanyName = soup.find('span', {'class': 'jobs-unified-top-card__company-name'}).text.strip()\n",
    "            jobLocation = driver.find_elements_by_class_name('jobs-unified-top-card__bullet')[0].get_attribute(\"innerHTML\")\n",
    "            jobType = soup.find('span', {'class': 'jobs-unified-top-card__workplace-type'}).text.strip()\n",
    "            jobPostedDate = soup.find('span', {'class': 'jobs-unified-top-card__posted-date'}).text.strip()\n",
    "            jobApplicantsRange = soup.find_all('span', {'class': 'jobs-unified-top-card__bullet'})[1].text.strip()\n",
    "            jobTotalApplicants = soup.find('li', {'class': 'jobs-unified-top-card__job-insight--highlight'}).text.strip()\n",
    "            jobLevel = soup.find_all('li', {'class': 'jobs-unified-top-card__job-insight'})[0].text.strip()\n",
    "            jobCompSize = soup.find_all('li', {'class': 'jobs-unified-top-card__job-insight'})[1].text.strip()\n",
    "        except:\n",
    "            jobCompanyName = ''\n",
    "            jobLocation = ''\n",
    "            jobType = ''\n",
    "            jobPostedDate = ''\n",
    "            jobApplicantsRange = ''\n",
    "            jobTotalApplicants =  ''\n",
    "            jobLevel = ''\n",
    "            jobCompSize = ''\n",
    "    except:\n",
    "        print(\"URL no longer available\")\n",
    "        jobURL = links\n",
    "        URLStatus = 'no longer active'\n",
    "        jobTitle = ''\n",
    "        jobDescription = ''\n",
    "        jobCompanyName = ''\n",
    "        jobLocation = ''\n",
    "        jobType = ''\n",
    "        jobPostedDate = ''\n",
    "        jobApplicantsRange = ''\n",
    "        jobTotalApplicants =  ''\n",
    "        jobLevel = ''\n",
    "        jobCompSize = ''\n",
    "\n",
    "    time.sleep(randTime)\n",
    "    extractDict = dict(((i, eval(i)) for i in ('jobURL', 'jobTitle', 'jobDescription', 'jobCompanyName', 'jobLocation', 'jobType', 'jobLevel', 'jobCompSize', 'jobPostedDate', 'jobApplicantsRange', 'jobTotalApplicants', 'URLStatus')))\n",
    "    final_dict.append(extractDict)\n",
    "    print(f\"links scrapped: {str(len(final_dict))}\")\n",
    "    print(f\"overall completion: %{str(round((len(final_dict) / len(full_url_csv) * 100),2))}\")\n",
    "    end_time = time.time()\n",
    "    jobPageTime+= end_time-start_time\n",
    "    print(f\"elapsed time: {str(round(jobPageTime,2))} seconds\" )\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "\n",
    "final_export = pd.DataFrame(final_dict)\n",
    "final_export.to_csv(f\"lnkd_scrapping_jobs_[{len(full_url_csv)}].csv\", index=False, header=True)\n",
    "end_time_cell = time.time()\n",
    "print(f\"Total elapsed time: {round((end_time_cell - start_time_cell) / 60,2)} minutes\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49c9f77a",
   "language": "python",
   "display_name": "PyCharm (_repo)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}