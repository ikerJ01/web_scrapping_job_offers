{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Job Offers web-scrapping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automation 1: Log-in Action"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    > This section automates the process of Logging-in to Linkedin.\n",
    "    > Make sure to use a valid Email Address and password.\n",
    "    > Selenium will open a chrome window which you can interact with.\n",
    "    > Once logged in, the site may ask to verify if you are human, you can use the pop-up window to solve the captcha or insert the access code.\n",
    "    > If any error arises, make sure to check the relevant driver.find_elements, as with time these are likely to be updated by Linkedin.\n",
    "'''\n",
    "\n",
    "# Creating a webdriver instance to log into LinkedIn using selenium required Chrome version\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Opening linkedIn's login page\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "\n",
    "# waiting for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Accessing username field\n",
    "username = driver.find_element_by_id(\"username\")\n",
    "\n",
    "# Enter Your Email Address\n",
    "username.send_keys(\"email@emailprovider.com\")\n",
    "\n",
    "# Accessing password fiel\n",
    "pword = driver.find_element_by_id(\"password\")\n",
    "\n",
    "# Enter Your Password\n",
    "pword.send_keys(\"PASSWORD\")\n",
    "\n",
    "# Clicking on the log in button\n",
    "driver.find_element_by_xpath(\"//button[@type='submit']\").click()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automation 2: Navigate to job section and insert job queries (Job Title and location)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    > This section automates the process to access the required job listings.\n",
    "    > If any error arises, make sure to check the relevant driver.find_elements as with time these are likely to be updated by Linkedin\n",
    "'''\n",
    "\n",
    "jobTitle = 'computer science'                   # Insert job title to query\n",
    "jobLocation = 'United Arab Emirates'            # Insert job location to query\n",
    "\n",
    "# Switching to Jobs page to be able to query location\n",
    "jobs = driver.find_element_by_xpath('//*[@id=\"global-nav\"]/div/nav/ul/li[3]')\n",
    "jobs.click()\n",
    "\n",
    "# Clicking on search bar\n",
    "searchBar = driver.find_element_by_class_name('jobs-search-box__keywords-label')\n",
    "searchBar.click()\n",
    "search_src = driver.page_source\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#Adds Job Title\n",
    "\n",
    "patternCriteria = re.compile(r'jobs-search-box-keyword-id-ember[0-9]*')\n",
    "matchCriteria = patternCriteria.search(search_src).group(0)\n",
    "searchKeyWords = driver.find_element_by_id(matchCriteria).send_keys(jobTitle)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#Adds Location and hits enter\n",
    "patternLocation = re.compile(r'jobs-search-box-location-id-ember[0-9]*')\n",
    "matchLocation = patternLocation.search(search_src).group(0)\n",
    "CountryKeyWords = driver.find_element_by_id(matchLocation).send_keys(jobLocation)\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_id(matchLocation).send_keys(Keys.ENTER)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automation 3: extract URLs from job offers list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    > This process will automatically scroll through the different pages available and extract all the URLs from the different job posts.\n",
    "    > Upon completion and as a checkpoint of the process, all links will be saved to disk in a CSV format in the project directory.\n",
    "    > Different validations are in place to have an overview of the extraction process and supervise the workflow.\n",
    "    > If any error arises, make sure to check the relevant driver.find_elements as with time these are likely to be updated by Linkedin.\n",
    "'''\n",
    "# Settings: Scroll through Pagination\n",
    "findPages = driver.find_elements_by_class_name(\"artdeco-pagination__indicator.artdeco-pagination__indicator--number\")\n",
    "totalPages = findPages[len(findPages)-1].text\n",
    "totalPagesInt = int(re.sub(r\"[^\\d.]\", \"\", totalPages))\n",
    "\n",
    "# Variable Setup for URL Extraction Loop\n",
    "indexPage = 1\n",
    "jobLoopTTime = 0\n",
    "job_links = []\n",
    "randTime = random.randint(1,6)\n",
    "extractedURL = []\n",
    "final_extractedURL = []\n",
    "\n",
    "# URL Extraction Loop\n",
    "for page in  range(0,totalPagesInt):\n",
    "    jobLoop_start_time = time.time()\n",
    "    time.sleep(randTime)\n",
    "\n",
    "    # Scroll down to load all available jobs to reach job list bottom line\n",
    "    jobsBlock = driver.find_element_by_class_name('scaffold-layout__list-container')  #Indicates container of job list\n",
    "    jobsScaffold = jobsBlock.find_elements_by_css_selector(\".jobs-search-results__list-item\")  #Indicates css selector for the job entries (can be pulled from first *li* element)\n",
    "    jobEntry = 0\n",
    "    for i in jobsScaffold:\n",
    "        jobEntry+= 1\n",
    "        print(f\"-- scrolling to job: {str(jobEntry)} --\")\n",
    "        driver.execute_script('arguments[0].scrollIntoView();', jobsScaffold[jobEntry-1])\n",
    "\n",
    "    #Scraps all links in job list\n",
    "    time.sleep(randTime)\n",
    "    job_src = driver.page_source\n",
    "    soup = BeautifulSoup(job_src, 'lxml')\n",
    "    for link in soup.find_all('a', {'class': 'job-card-list__title'}):\n",
    "        job_links.append(link.get('href'))\n",
    "        URL = link.get('href')\n",
    "        pageNumb = indexPage\n",
    "        extractedURL = dict(((i, eval(i)) for i in ('pageNumb', 'URL')))\n",
    "        final_extractedURL.append(extractedURL)\n",
    "    print(job_links)\n",
    "    print(extractedURL)\n",
    "    print(f\"links extracted: {str(len(job_links))}\")\n",
    "    print(f\"pages scrapped: {str(round(len(job_links) / 25))}\")\n",
    "    print(f\"Duplicate Links Status: {str('NO Duplicated links' if (len(job_links) == len(set(job_links))) else 'There ARE Duplicates')}\")\n",
    "\n",
    "    #Scrolls to next page\n",
    "    indexPage+= 1\n",
    "    getNextPage = driver.find_element_by_xpath(\"//button[@aria-label='Page \"+str(indexPage)+\"']\")\n",
    "    getNextPage.send_keys(Keys.RETURN)\n",
    "    time.sleep(randTime)\n",
    "    print(f\"Next page is: {str(indexPage)}\")\n",
    "    jobLoop_end_time = time.time()\n",
    "    jobLoopTTime+= jobLoop_end_time - jobLoop_start_time\n",
    "    print(f\"elapsed time: {str(round(jobLoopTTime,3))} seconds\")\n",
    "    print(\"________________\")\n",
    "\n",
    "print(f'Total Elapsed Time: {str(round(jobLoopTTime/60,3))} minutes')\n",
    "\n",
    "# Adding main URL to extracted links\n",
    "full_url = [('https://linkedin.com' + URL) for URL in job_links]\n",
    "\n",
    "#Saving to disk the extracted links as backup\n",
    "URLsExport = pd.DataFrame(set(full_url))\n",
    "URLsExport.to_csv(\"URLscrapped.csv\", index=False, header=False)\n",
    "\n",
    "'''\n",
    "    > Uncomment below lines for debugging duplicates, URLsExport_2 stores both the URL and the page where the link was extracted from.\n",
    "'''\n",
    "# URLsExport_2 = pd.DataFrame(final_extractedURL)\n",
    "#URLsExport_2.to_csv(\"URLscrapped_bypage.csv\", index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automation 4: Extract job offer details"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    > This process will use the links saved in the CSV and not the ones stored in memory.\n",
    "'''\n",
    "#extract links from CSV\n",
    "full_url_csv = pd.read_csv('URLscrapped.csv', names=['link'])\n",
    "full_url_csv = full_url_csv['link'].tolist()\n",
    "print(f'Total URLs loaded from file: {len(full_url_csv)} links')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    > Uncomment below section if you want to scrap the job details in batches.\n",
    "    > Recommended if you are modifying the code and want to test the updates.\n",
    "'''\n",
    "# Spliting links into different groups to run the scrap in batches\n",
    "\n",
    "#test_urls_1 = full_url_csv[0:100]\n",
    "#test_urls_2 = full_url_csv[101:200]\n",
    "#test_urls_3 = full_url_csv[201:300]\n",
    "#test_urls_4 = full_url_csv[301:400]\n",
    "#test_urls_5 = full_url_csv[401:500]\n",
    "#test_urls_6 = full_url_csv[501:600]\n",
    "#test_urls_7 = full_url_csv[601:700]\n",
    "#test_urls_8 = full_url_csv[701:800]\n",
    "#test_urls_9 = full_url_csv[801:900]\n",
    "#test_urls_10 = full_url_csv[901:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#variable to insert dictionary of elements extracted\n",
    "final_dict = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "    > The following loop will automate the extraction process from the loaded URLs.\n",
    "    > Extracted details will be saved to lnkd_scrapping_jobs.csv\n",
    "    > Different validations are in place to have an overview of the extraction process and supervise the workflow.\n",
    "    > If any error arises, make sure to check the relevant driver.find_elements as with time these are likely to be updated by Linkedin.\n",
    "    > If a link is no longer available, the fields for that given URL will be saved as empty fields and status will be set to: no longer active.\n",
    "'''\n",
    "# Loop to extract details from URL page\n",
    "start_time_cell = time.time()\n",
    "indexPage = 0\n",
    "jobLoopTTime = 0\n",
    "job_links = []\n",
    "randTime = random.randint(1,6)\n",
    "jobPageTime = 0\n",
    "\n",
    "for links in full_url_csv:\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        driver.get(links)\n",
    "        time.sleep(randTime)\n",
    "        jobDetails_src = driver.page_source\n",
    "        driver.find_element_by_class_name(\"artdeco-card__action\").click()\n",
    "        soup = BeautifulSoup(jobDetails_src, 'lxml')\n",
    "\n",
    "        jobURL = links\n",
    "        jobTitle = soup.find('h1', {'class': 'jobs-unified-top-card__job-title'}).text.strip()\n",
    "        jobDescription = soup.find('div', {'class': 'jobs-description-content__text--stretch'}).text.strip()\n",
    "        URLStatus = 'Active'\n",
    "        try:\n",
    "            jobCompanyName = soup.find('span', {'class': 'jobs-unified-top-card__company-name'}).text.strip()\n",
    "            jobLocation = driver.find_elements_by_class_name('jobs-unified-top-card__bullet')[0].get_attribute(\"innerHTML\")\n",
    "            jobType = soup.find('span', {'class': 'jobs-unified-top-card__workplace-type'}).text.strip()\n",
    "            jobPostedDate = soup.find('span', {'class': 'jobs-unified-top-card__posted-date'}).text.strip()\n",
    "            jobApplicantsRange = soup.find_all('span', {'class': 'jobs-unified-top-card__bullet'})[1].text.strip()\n",
    "            jobTotalApplicants = soup.find('li', {'class': 'jobs-unified-top-card__job-insight--highlight'}).text.strip()\n",
    "            jobLevel = soup.find_all('li', {'class': 'jobs-unified-top-card__job-insight'})[0].text.strip()\n",
    "            jobCompSize = soup.find_all('li', {'class': 'jobs-unified-top-card__job-insight'})[1].text.strip()\n",
    "        except:\n",
    "            jobCompanyName = ''\n",
    "            jobLocation = ''\n",
    "            jobType = ''\n",
    "            jobPostedDate = ''\n",
    "            jobApplicantsRange = ''\n",
    "            jobTotalApplicants =  ''\n",
    "            jobLevel = ''\n",
    "            jobCompSize = ''\n",
    "    except:\n",
    "        print(\"URL no longer available\")\n",
    "        jobURL = links\n",
    "        URLStatus = 'no longer active'\n",
    "        jobTitle = ''\n",
    "        jobDescription = ''\n",
    "        jobCompanyName = ''\n",
    "        jobLocation = ''\n",
    "        jobType = ''\n",
    "        jobPostedDate = ''\n",
    "        jobApplicantsRange = ''\n",
    "        jobTotalApplicants =  ''\n",
    "        jobLevel = ''\n",
    "        jobCompSize = ''\n",
    "\n",
    "    time.sleep(randTime)\n",
    "    extractDict = dict(((i, eval(i)) for i in ('jobURL', 'jobTitle', 'jobDescription', 'jobCompanyName', 'jobLocation', 'jobType', 'jobLevel', 'jobCompSize', 'jobPostedDate', 'jobApplicantsRange', 'jobTotalApplicants', 'URLStatus')))\n",
    "    final_dict.append(extractDict)\n",
    "    print(f\"links scrapped: {str(len(final_dict))}\")\n",
    "    print(f\"overall completion: %{str(round((len(final_dict) / len(full_url_csv) * 100),2))}\")\n",
    "    end_time = time.time()\n",
    "    jobPageTime+= end_time-start_time\n",
    "    print(f\"elapsed time: {str(round(jobPageTime,2))} seconds\" )\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "\n",
    "final_export = pd.DataFrame(final_dict)\n",
    "final_export.to_csv(f\"lnkd_scrapping_jobs_[{len(full_url_csv)}].csv\", index=False, header=True)\n",
    "end_time_cell = time.time()\n",
    "print(f\"Total elapsed time: {round((end_time_cell - start_time_cell) / 60,2)} minutes\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49c9f77a",
   "language": "python",
   "display_name": "PyCharm (_repo)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}